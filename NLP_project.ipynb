{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPiSCs8GPlc2SlAz4G3BW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreenGoose19/Next_Word_Prediction/blob/main/NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pNY6u2frDrPy"
      },
      "outputs": [],
      "source": [
        "corpus=open(\"corpus.txt\").read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dZVPsQ0WGF0W",
        "outputId": "fcb16735-1999-4003-f0fa-2519eab4296c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lagers and ales are two distinct categories of beer, each with its own unique characteristics and brewing processes. Lagers are typically known for their crisp and clean flavors, achieved through a cold fermentation process that uses bottom-fermenting yeast strains. This cold fermentation allows for a longer and slower maturation period, resulting in a smoother and often lighter-bodied beer. On the other hand, ales are brewed with top-fermenting yeast strains at warmer temperatures, which leads to a faster fermentation process and a wider range of flavors and aromas. Ales can vary from pale and hoppy India Pale Ales (IPAs) to rich and malty stouts, providing a diverse array of taste experiences. Both lagers and ales have their own dedicated fan bases and are enjoyed around the world, making them integral parts of the vibrant and ever-evolving world of craft beer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the corpus\n",
        "import re\n",
        "corpus=corpus.lower()\n",
        "clean_corpus=re.sub('[^a-z0-9]+',' ', corpus)"
      ],
      "metadata": {
        "id": "jV4mLD3cGNsR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "jTob9yLLGSGM",
        "outputId": "a34600ff-e831-4ef0-e2d7-7135950abd9f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lagers and ales are two distinct categories of beer each with its own unique characteristics and brewing processes lagers are typically known for their crisp and clean flavors achieved through a cold fermentation process that uses bottom fermenting yeast strains this cold fermentation allows for a longer and slower maturation period resulting in a smoother and often lighter bodied beer on the other hand ales are brewed with top fermenting yeast strains at warmer temperatures which leads to a faster fermentation process and a wider range of flavors and aromas ales can vary from pale and hoppy india pale ales ipas to rich and malty stouts providing a diverse array of taste experiences both lagers and ales have their own dedicated fan bases and are enjoyed around the world making them integral parts of the vibrant and ever evolving world of craft beer '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#required libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRDYnt3KNsDE",
        "outputId": "a1773d99-eb64-4613-f5c6-a9d839e139ed"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the text into words\n",
        "tokens = word_tokenize(clean_corpus)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6ZLyIiwNxCQ",
        "outputId": "2055fe89-13ab-4e04-9fbc-59d005b27394"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lagers',\n",
              " 'and',\n",
              " 'ales',\n",
              " 'are',\n",
              " 'two',\n",
              " 'distinct',\n",
              " 'categories',\n",
              " 'of',\n",
              " 'beer',\n",
              " 'each',\n",
              " 'with',\n",
              " 'its',\n",
              " 'own',\n",
              " 'unique',\n",
              " 'characteristics',\n",
              " 'and',\n",
              " 'brewing',\n",
              " 'processes',\n",
              " 'lagers',\n",
              " 'are',\n",
              " 'typically',\n",
              " 'known',\n",
              " 'for',\n",
              " 'their',\n",
              " 'crisp',\n",
              " 'and',\n",
              " 'clean',\n",
              " 'flavors',\n",
              " 'achieved',\n",
              " 'through',\n",
              " 'a',\n",
              " 'cold',\n",
              " 'fermentation',\n",
              " 'process',\n",
              " 'that',\n",
              " 'uses',\n",
              " 'bottom',\n",
              " 'fermenting',\n",
              " 'yeast',\n",
              " 'strains',\n",
              " 'this',\n",
              " 'cold',\n",
              " 'fermentation',\n",
              " 'allows',\n",
              " 'for',\n",
              " 'a',\n",
              " 'longer',\n",
              " 'and',\n",
              " 'slower',\n",
              " 'maturation',\n",
              " 'period',\n",
              " 'resulting',\n",
              " 'in',\n",
              " 'a',\n",
              " 'smoother',\n",
              " 'and',\n",
              " 'often',\n",
              " 'lighter',\n",
              " 'bodied',\n",
              " 'beer',\n",
              " 'on',\n",
              " 'the',\n",
              " 'other',\n",
              " 'hand',\n",
              " 'ales',\n",
              " 'are',\n",
              " 'brewed',\n",
              " 'with',\n",
              " 'top',\n",
              " 'fermenting',\n",
              " 'yeast',\n",
              " 'strains',\n",
              " 'at',\n",
              " 'warmer',\n",
              " 'temperatures',\n",
              " 'which',\n",
              " 'leads',\n",
              " 'to',\n",
              " 'a',\n",
              " 'faster',\n",
              " 'fermentation',\n",
              " 'process',\n",
              " 'and',\n",
              " 'a',\n",
              " 'wider',\n",
              " 'range',\n",
              " 'of',\n",
              " 'flavors',\n",
              " 'and',\n",
              " 'aromas',\n",
              " 'ales',\n",
              " 'can',\n",
              " 'vary',\n",
              " 'from',\n",
              " 'pale',\n",
              " 'and',\n",
              " 'hoppy',\n",
              " 'india',\n",
              " 'pale',\n",
              " 'ales',\n",
              " 'ipas',\n",
              " 'to',\n",
              " 'rich',\n",
              " 'and',\n",
              " 'malty',\n",
              " 'stouts',\n",
              " 'providing',\n",
              " 'a',\n",
              " 'diverse',\n",
              " 'array',\n",
              " 'of',\n",
              " 'taste',\n",
              " 'experiences',\n",
              " 'both',\n",
              " 'lagers',\n",
              " 'and',\n",
              " 'ales',\n",
              " 'have',\n",
              " 'their',\n",
              " 'own',\n",
              " 'dedicated',\n",
              " 'fan',\n",
              " 'bases',\n",
              " 'and',\n",
              " 'are',\n",
              " 'enjoyed',\n",
              " 'around',\n",
              " 'the',\n",
              " 'world',\n",
              " 'making',\n",
              " 'them',\n",
              " 'integral',\n",
              " 'parts',\n",
              " 'of',\n",
              " 'the',\n",
              " 'vibrant',\n",
              " 'and',\n",
              " 'ever',\n",
              " 'evolving',\n",
              " 'world',\n",
              " 'of',\n",
              " 'craft',\n",
              " 'beer']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#length of the sequence to train\n",
        "train_len = 3\n",
        ""
      ],
      "metadata": {
        "id": "iugmTHaIN8WL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the data into required sequence\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)+1):\n",
        "  seq = tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)\n",
        ""
      ],
      "metadata": {
        "id": "E6XwrZB_N-yk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lzChNDqOCsQ",
        "outputId": "4c33fbf1-d79a-4760-aeec-42a00a86eccd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['lagers', 'and', 'ales'],\n",
              " ['and', 'ales', 'are'],\n",
              " ['ales', 'are', 'two'],\n",
              " ['are', 'two', 'distinct'],\n",
              " ['two', 'distinct', 'categories'],\n",
              " ['distinct', 'categories', 'of'],\n",
              " ['categories', 'of', 'beer'],\n",
              " ['of', 'beer', 'each'],\n",
              " ['beer', 'each', 'with'],\n",
              " ['each', 'with', 'its'],\n",
              " ['with', 'its', 'own'],\n",
              " ['its', 'own', 'unique'],\n",
              " ['own', 'unique', 'characteristics'],\n",
              " ['unique', 'characteristics', 'and'],\n",
              " ['characteristics', 'and', 'brewing'],\n",
              " ['and', 'brewing', 'processes'],\n",
              " ['brewing', 'processes', 'lagers'],\n",
              " ['processes', 'lagers', 'are'],\n",
              " ['lagers', 'are', 'typically'],\n",
              " ['are', 'typically', 'known'],\n",
              " ['typically', 'known', 'for'],\n",
              " ['known', 'for', 'their'],\n",
              " ['for', 'their', 'crisp'],\n",
              " ['their', 'crisp', 'and'],\n",
              " ['crisp', 'and', 'clean'],\n",
              " ['and', 'clean', 'flavors'],\n",
              " ['clean', 'flavors', 'achieved'],\n",
              " ['flavors', 'achieved', 'through'],\n",
              " ['achieved', 'through', 'a'],\n",
              " ['through', 'a', 'cold'],\n",
              " ['a', 'cold', 'fermentation'],\n",
              " ['cold', 'fermentation', 'process'],\n",
              " ['fermentation', 'process', 'that'],\n",
              " ['process', 'that', 'uses'],\n",
              " ['that', 'uses', 'bottom'],\n",
              " ['uses', 'bottom', 'fermenting'],\n",
              " ['bottom', 'fermenting', 'yeast'],\n",
              " ['fermenting', 'yeast', 'strains'],\n",
              " ['yeast', 'strains', 'this'],\n",
              " ['strains', 'this', 'cold'],\n",
              " ['this', 'cold', 'fermentation'],\n",
              " ['cold', 'fermentation', 'allows'],\n",
              " ['fermentation', 'allows', 'for'],\n",
              " ['allows', 'for', 'a'],\n",
              " ['for', 'a', 'longer'],\n",
              " ['a', 'longer', 'and'],\n",
              " ['longer', 'and', 'slower'],\n",
              " ['and', 'slower', 'maturation'],\n",
              " ['slower', 'maturation', 'period'],\n",
              " ['maturation', 'period', 'resulting'],\n",
              " ['period', 'resulting', 'in'],\n",
              " ['resulting', 'in', 'a'],\n",
              " ['in', 'a', 'smoother'],\n",
              " ['a', 'smoother', 'and'],\n",
              " ['smoother', 'and', 'often'],\n",
              " ['and', 'often', 'lighter'],\n",
              " ['often', 'lighter', 'bodied'],\n",
              " ['lighter', 'bodied', 'beer'],\n",
              " ['bodied', 'beer', 'on'],\n",
              " ['beer', 'on', 'the'],\n",
              " ['on', 'the', 'other'],\n",
              " ['the', 'other', 'hand'],\n",
              " ['other', 'hand', 'ales'],\n",
              " ['hand', 'ales', 'are'],\n",
              " ['ales', 'are', 'brewed'],\n",
              " ['are', 'brewed', 'with'],\n",
              " ['brewed', 'with', 'top'],\n",
              " ['with', 'top', 'fermenting'],\n",
              " ['top', 'fermenting', 'yeast'],\n",
              " ['fermenting', 'yeast', 'strains'],\n",
              " ['yeast', 'strains', 'at'],\n",
              " ['strains', 'at', 'warmer'],\n",
              " ['at', 'warmer', 'temperatures'],\n",
              " ['warmer', 'temperatures', 'which'],\n",
              " ['temperatures', 'which', 'leads'],\n",
              " ['which', 'leads', 'to'],\n",
              " ['leads', 'to', 'a'],\n",
              " ['to', 'a', 'faster'],\n",
              " ['a', 'faster', 'fermentation'],\n",
              " ['faster', 'fermentation', 'process'],\n",
              " ['fermentation', 'process', 'and'],\n",
              " ['process', 'and', 'a'],\n",
              " ['and', 'a', 'wider'],\n",
              " ['a', 'wider', 'range'],\n",
              " ['wider', 'range', 'of'],\n",
              " ['range', 'of', 'flavors'],\n",
              " ['of', 'flavors', 'and'],\n",
              " ['flavors', 'and', 'aromas'],\n",
              " ['and', 'aromas', 'ales'],\n",
              " ['aromas', 'ales', 'can'],\n",
              " ['ales', 'can', 'vary'],\n",
              " ['can', 'vary', 'from'],\n",
              " ['vary', 'from', 'pale'],\n",
              " ['from', 'pale', 'and'],\n",
              " ['pale', 'and', 'hoppy'],\n",
              " ['and', 'hoppy', 'india'],\n",
              " ['hoppy', 'india', 'pale'],\n",
              " ['india', 'pale', 'ales'],\n",
              " ['pale', 'ales', 'ipas'],\n",
              " ['ales', 'ipas', 'to'],\n",
              " ['ipas', 'to', 'rich'],\n",
              " ['to', 'rich', 'and'],\n",
              " ['rich', 'and', 'malty'],\n",
              " ['and', 'malty', 'stouts'],\n",
              " ['malty', 'stouts', 'providing'],\n",
              " ['stouts', 'providing', 'a'],\n",
              " ['providing', 'a', 'diverse'],\n",
              " ['a', 'diverse', 'array'],\n",
              " ['diverse', 'array', 'of'],\n",
              " ['array', 'of', 'taste'],\n",
              " ['of', 'taste', 'experiences'],\n",
              " ['taste', 'experiences', 'both'],\n",
              " ['experiences', 'both', 'lagers'],\n",
              " ['both', 'lagers', 'and'],\n",
              " ['lagers', 'and', 'ales'],\n",
              " ['and', 'ales', 'have'],\n",
              " ['ales', 'have', 'their'],\n",
              " ['have', 'their', 'own'],\n",
              " ['their', 'own', 'dedicated'],\n",
              " ['own', 'dedicated', 'fan'],\n",
              " ['dedicated', 'fan', 'bases'],\n",
              " ['fan', 'bases', 'and'],\n",
              " ['bases', 'and', 'are'],\n",
              " ['and', 'are', 'enjoyed'],\n",
              " ['are', 'enjoyed', 'around'],\n",
              " ['enjoyed', 'around', 'the'],\n",
              " ['around', 'the', 'world'],\n",
              " ['the', 'world', 'making'],\n",
              " ['world', 'making', 'them'],\n",
              " ['making', 'them', 'integral'],\n",
              " ['them', 'integral', 'parts'],\n",
              " ['integral', 'parts', 'of'],\n",
              " ['parts', 'of', 'the'],\n",
              " ['of', 'the', 'vibrant'],\n",
              " ['the', 'vibrant', 'and'],\n",
              " ['vibrant', 'and', 'ever'],\n",
              " ['and', 'ever', 'evolving'],\n",
              " ['ever', 'evolving', 'world'],\n",
              " ['evolving', 'world', 'of'],\n",
              " ['world', 'of', 'craft'],\n",
              " ['of', 'craft', 'beer']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the texts into integer sequence\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLuFQLvyOFqR",
        "outputId": "3a5268b8-4169-43b8-f4ea-db720c2fe804"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 1, 3],\n",
              " [1, 3, 5],\n",
              " [3, 5, 23],\n",
              " [5, 23, 24],\n",
              " [23, 24, 25],\n",
              " [24, 25, 4],\n",
              " [25, 4, 9],\n",
              " [4, 9, 26],\n",
              " [9, 26, 10],\n",
              " [26, 10, 27],\n",
              " [10, 27, 11],\n",
              " [27, 11, 28],\n",
              " [11, 28, 29],\n",
              " [28, 29, 1],\n",
              " [29, 1, 30],\n",
              " [1, 30, 31],\n",
              " [30, 31, 8],\n",
              " [31, 8, 5],\n",
              " [8, 5, 32],\n",
              " [5, 32, 33],\n",
              " [32, 33, 12],\n",
              " [33, 12, 13],\n",
              " [12, 13, 34],\n",
              " [13, 34, 1],\n",
              " [34, 1, 35],\n",
              " [1, 35, 14],\n",
              " [35, 14, 36],\n",
              " [14, 36, 37],\n",
              " [36, 37, 2],\n",
              " [37, 2, 15],\n",
              " [2, 15, 6],\n",
              " [15, 6, 16],\n",
              " [6, 16, 38],\n",
              " [16, 38, 39],\n",
              " [38, 39, 40],\n",
              " [39, 40, 17],\n",
              " [40, 17, 18],\n",
              " [17, 18, 19],\n",
              " [18, 19, 41],\n",
              " [19, 41, 15],\n",
              " [41, 15, 6],\n",
              " [15, 6, 42],\n",
              " [6, 42, 12],\n",
              " [42, 12, 2],\n",
              " [12, 2, 43],\n",
              " [2, 43, 1],\n",
              " [43, 1, 44],\n",
              " [1, 44, 45],\n",
              " [44, 45, 46],\n",
              " [45, 46, 47],\n",
              " [46, 47, 48],\n",
              " [47, 48, 2],\n",
              " [48, 2, 49],\n",
              " [2, 49, 1],\n",
              " [49, 1, 50],\n",
              " [1, 50, 51],\n",
              " [50, 51, 52],\n",
              " [51, 52, 9],\n",
              " [52, 9, 53],\n",
              " [9, 53, 7],\n",
              " [53, 7, 54],\n",
              " [7, 54, 55],\n",
              " [54, 55, 3],\n",
              " [55, 3, 5],\n",
              " [3, 5, 56],\n",
              " [5, 56, 10],\n",
              " [56, 10, 57],\n",
              " [10, 57, 17],\n",
              " [57, 17, 18],\n",
              " [17, 18, 19],\n",
              " [18, 19, 58],\n",
              " [19, 58, 59],\n",
              " [58, 59, 60],\n",
              " [59, 60, 61],\n",
              " [60, 61, 62],\n",
              " [61, 62, 20],\n",
              " [62, 20, 2],\n",
              " [20, 2, 63],\n",
              " [2, 63, 6],\n",
              " [63, 6, 16],\n",
              " [6, 16, 1],\n",
              " [16, 1, 2],\n",
              " [1, 2, 64],\n",
              " [2, 64, 65],\n",
              " [64, 65, 4],\n",
              " [65, 4, 14],\n",
              " [4, 14, 1],\n",
              " [14, 1, 66],\n",
              " [1, 66, 3],\n",
              " [66, 3, 67],\n",
              " [3, 67, 68],\n",
              " [67, 68, 69],\n",
              " [68, 69, 21],\n",
              " [69, 21, 1],\n",
              " [21, 1, 70],\n",
              " [1, 70, 71],\n",
              " [70, 71, 21],\n",
              " [71, 21, 3],\n",
              " [21, 3, 72],\n",
              " [3, 72, 20],\n",
              " [72, 20, 73],\n",
              " [20, 73, 1],\n",
              " [73, 1, 74],\n",
              " [1, 74, 75],\n",
              " [74, 75, 76],\n",
              " [75, 76, 2],\n",
              " [76, 2, 77],\n",
              " [2, 77, 78],\n",
              " [77, 78, 4],\n",
              " [78, 4, 79],\n",
              " [4, 79, 80],\n",
              " [79, 80, 81],\n",
              " [80, 81, 8],\n",
              " [81, 8, 1],\n",
              " [8, 1, 3],\n",
              " [1, 3, 82],\n",
              " [3, 82, 13],\n",
              " [82, 13, 11],\n",
              " [13, 11, 83],\n",
              " [11, 83, 84],\n",
              " [83, 84, 85],\n",
              " [84, 85, 1],\n",
              " [85, 1, 5],\n",
              " [1, 5, 86],\n",
              " [5, 86, 87],\n",
              " [86, 87, 7],\n",
              " [87, 7, 22],\n",
              " [7, 22, 88],\n",
              " [22, 88, 89],\n",
              " [88, 89, 90],\n",
              " [89, 90, 91],\n",
              " [90, 91, 4],\n",
              " [91, 4, 7],\n",
              " [4, 7, 92],\n",
              " [7, 92, 1],\n",
              " [92, 1, 93],\n",
              " [1, 93, 94],\n",
              " [93, 94, 22],\n",
              " [94, 22, 4],\n",
              " [22, 4, 95],\n",
              " [4, 95, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=np.asarray(sequences)"
      ],
      "metadata": {
        "id": "9uiWyhLWOMJ_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary size\n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPaB1nz_OOmF",
        "outputId": "042b923c-5edc-4ccb-dd81-35befadbabf1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainX\n",
        "train_inputs=sequences[:,:-1]\n",
        ""
      ],
      "metadata": {
        "id": "Gm9dEcP1Od1R"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTQKt08hOfno",
        "outputId": "fabed5e8-566f-49f2-b420-6bd48dc302b2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8,  1],\n",
              "       [ 1,  3],\n",
              "       [ 3,  5],\n",
              "       [ 5, 23],\n",
              "       [23, 24],\n",
              "       [24, 25],\n",
              "       [25,  4],\n",
              "       [ 4,  9],\n",
              "       [ 9, 26],\n",
              "       [26, 10],\n",
              "       [10, 27],\n",
              "       [27, 11],\n",
              "       [11, 28],\n",
              "       [28, 29],\n",
              "       [29,  1],\n",
              "       [ 1, 30],\n",
              "       [30, 31],\n",
              "       [31,  8],\n",
              "       [ 8,  5],\n",
              "       [ 5, 32],\n",
              "       [32, 33],\n",
              "       [33, 12],\n",
              "       [12, 13],\n",
              "       [13, 34],\n",
              "       [34,  1],\n",
              "       [ 1, 35],\n",
              "       [35, 14],\n",
              "       [14, 36],\n",
              "       [36, 37],\n",
              "       [37,  2],\n",
              "       [ 2, 15],\n",
              "       [15,  6],\n",
              "       [ 6, 16],\n",
              "       [16, 38],\n",
              "       [38, 39],\n",
              "       [39, 40],\n",
              "       [40, 17],\n",
              "       [17, 18],\n",
              "       [18, 19],\n",
              "       [19, 41],\n",
              "       [41, 15],\n",
              "       [15,  6],\n",
              "       [ 6, 42],\n",
              "       [42, 12],\n",
              "       [12,  2],\n",
              "       [ 2, 43],\n",
              "       [43,  1],\n",
              "       [ 1, 44],\n",
              "       [44, 45],\n",
              "       [45, 46],\n",
              "       [46, 47],\n",
              "       [47, 48],\n",
              "       [48,  2],\n",
              "       [ 2, 49],\n",
              "       [49,  1],\n",
              "       [ 1, 50],\n",
              "       [50, 51],\n",
              "       [51, 52],\n",
              "       [52,  9],\n",
              "       [ 9, 53],\n",
              "       [53,  7],\n",
              "       [ 7, 54],\n",
              "       [54, 55],\n",
              "       [55,  3],\n",
              "       [ 3,  5],\n",
              "       [ 5, 56],\n",
              "       [56, 10],\n",
              "       [10, 57],\n",
              "       [57, 17],\n",
              "       [17, 18],\n",
              "       [18, 19],\n",
              "       [19, 58],\n",
              "       [58, 59],\n",
              "       [59, 60],\n",
              "       [60, 61],\n",
              "       [61, 62],\n",
              "       [62, 20],\n",
              "       [20,  2],\n",
              "       [ 2, 63],\n",
              "       [63,  6],\n",
              "       [ 6, 16],\n",
              "       [16,  1],\n",
              "       [ 1,  2],\n",
              "       [ 2, 64],\n",
              "       [64, 65],\n",
              "       [65,  4],\n",
              "       [ 4, 14],\n",
              "       [14,  1],\n",
              "       [ 1, 66],\n",
              "       [66,  3],\n",
              "       [ 3, 67],\n",
              "       [67, 68],\n",
              "       [68, 69],\n",
              "       [69, 21],\n",
              "       [21,  1],\n",
              "       [ 1, 70],\n",
              "       [70, 71],\n",
              "       [71, 21],\n",
              "       [21,  3],\n",
              "       [ 3, 72],\n",
              "       [72, 20],\n",
              "       [20, 73],\n",
              "       [73,  1],\n",
              "       [ 1, 74],\n",
              "       [74, 75],\n",
              "       [75, 76],\n",
              "       [76,  2],\n",
              "       [ 2, 77],\n",
              "       [77, 78],\n",
              "       [78,  4],\n",
              "       [ 4, 79],\n",
              "       [79, 80],\n",
              "       [80, 81],\n",
              "       [81,  8],\n",
              "       [ 8,  1],\n",
              "       [ 1,  3],\n",
              "       [ 3, 82],\n",
              "       [82, 13],\n",
              "       [13, 11],\n",
              "       [11, 83],\n",
              "       [83, 84],\n",
              "       [84, 85],\n",
              "       [85,  1],\n",
              "       [ 1,  5],\n",
              "       [ 5, 86],\n",
              "       [86, 87],\n",
              "       [87,  7],\n",
              "       [ 7, 22],\n",
              "       [22, 88],\n",
              "       [88, 89],\n",
              "       [89, 90],\n",
              "       [90, 91],\n",
              "       [91,  4],\n",
              "       [ 4,  7],\n",
              "       [ 7, 92],\n",
              "       [92,  1],\n",
              "       [ 1, 93],\n",
              "       [93, 94],\n",
              "       [94, 22],\n",
              "       [22,  4],\n",
              "       [ 4, 95]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input sequence length\n",
        "seq_length=train_inputs.shape[1]\n",
        "seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKBaydOzOkLs",
        "outputId": "1c28813b-04a5-4866-bb4f-2ea70cc16aec"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainY\n",
        "train_targets=sequences[:,-1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "stcQ9UrkPIA-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaF41CNaPKJq",
        "outputId": "2e5faac7-1dc4-43b5-9bb0-ff3ba7c1e7ed"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  5, 23, 24, 25,  4,  9, 26, 10, 27, 11, 28, 29,  1, 30, 31,  8,\n",
              "        5, 32, 33, 12, 13, 34,  1, 35, 14, 36, 37,  2, 15,  6, 16, 38, 39,\n",
              "       40, 17, 18, 19, 41, 15,  6, 42, 12,  2, 43,  1, 44, 45, 46, 47, 48,\n",
              "        2, 49,  1, 50, 51, 52,  9, 53,  7, 54, 55,  3,  5, 56, 10, 57, 17,\n",
              "       18, 19, 58, 59, 60, 61, 62, 20,  2, 63,  6, 16,  1,  2, 64, 65,  4,\n",
              "       14,  1, 66,  3, 67, 68, 69, 21,  1, 70, 71, 21,  3, 72, 20, 73,  1,\n",
              "       74, 75, 76,  2, 77, 78,  4, 79, 80, 81,  8,  1,  3, 82, 13, 11, 83,\n",
              "       84, 85,  1,  5, 86, 87,  7, 22, 88, 89, 90, 91,  4,  7, 92,  1, 93,\n",
              "       94, 22,  4, 95,  9])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)"
      ],
      "metadata": {
        "id": "UszISublPMJI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNxGSDyvPRcV",
        "outputId": "13c2ec2e-f40c-4dc9-f7fd-35a7a1e65db1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL** **BUILDING**"
      ],
      "metadata": {
        "id": "jDUIUacWPZRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#required libraries\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "tXWllJGGPg1x"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm model\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        #simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=False)\n",
        "\n",
        "        #fully connected layer\n",
        "        self.linear = nn.Linear(hidden_size*seq_length,vocab_size)\n",
        "\n",
        "    def forward(self, input_word):\n",
        "        #input sequence to embeddings\n",
        "        embedded = self.embed(input_word)\n",
        "\n",
        "        #passing the embedding to lstm model\n",
        "        output, hidden = self.lstm(embedded)\n",
        "\n",
        "        #reshaping\n",
        "        output=output.view(output.size(0), -1)\n",
        "\n",
        "        #fully connected layer\n",
        "        output = self.linear(output)\n",
        "        return output,hidden\n"
      ],
      "metadata": {
        "id": "0FGRAcoRPjb0"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=lstm(vocab_size=vocabulary_size,embed_size=128, hidden_size=256)\n",
        ""
      ],
      "metadata": {
        "id": "Eu_gQEAsP7JZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_1UCJW-P8uh",
        "outputId": "d22edd24-56bb-4e01-dc63-9a17bb209edc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lstm(\n",
              "  (embed): Embedding(96, 128)\n",
              "  (lstm): LSTM(128, 256, num_layers=2)\n",
              "  (linear): Linear(in_features=512, out_features=96, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adam optimizer\n",
        "optimizer= Adam(model.parameters(), lr=0.07)\n",
        "\n",
        "#loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#training the model\n",
        "def train(epoch):\n",
        "    #set the model to train\n",
        "    model.train()\n",
        "    tr_loss=0\n",
        "\n",
        "    #clearing the Gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #predict the output\n",
        "    y_pred, (state_h, state_c) = model(torch.from_numpy(train_inputs))\n",
        "\n",
        "    #compute the loss\n",
        "    loss=criterion(y_pred,torch.from_numpy(train_targets))\n",
        "    losses.append(loss)\n",
        "\n",
        "    #backpropagate\n",
        "    loss.backward()\n",
        "\n",
        "    #update the parameters\n",
        "    optimizer.step()\n",
        "    tr_loss = loss.item()\n",
        "\n",
        "    print(\"Epoch : \",epoch,\"loss : \",loss)"
      ],
      "metadata": {
        "id": "ZZmyspEjQEXN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of epoch\n",
        "no_epoch=50\n",
        "losses=[]\n",
        "for epoch in range(1,no_epoch+1):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTqW-8ebQIvI",
        "outputId": "502b2a61-30c4-45e7-a4df-0fbfe4579fe1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1 loss :  tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  2 loss :  tensor(0.1833, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  3 loss :  tensor(0.3825, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  4 loss :  tensor(0.9814, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  5 loss :  tensor(0.5652, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  6 loss :  tensor(0.2225, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  7 loss :  tensor(0.1565, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  8 loss :  tensor(0.1675, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  9 loss :  tensor(0.1701, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  10 loss :  tensor(0.1864, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  11 loss :  tensor(0.1944, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  12 loss :  tensor(0.2166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  13 loss :  tensor(0.2281, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  14 loss :  tensor(0.2318, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  15 loss :  tensor(0.2245, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  16 loss :  tensor(0.2497, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  17 loss :  tensor(0.2229, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  18 loss :  tensor(0.2235, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  19 loss :  tensor(0.2237, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  20 loss :  tensor(0.2283, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  21 loss :  tensor(0.2103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  22 loss :  tensor(0.2147, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  23 loss :  tensor(0.2092, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  24 loss :  tensor(0.1916, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  25 loss :  tensor(0.1825, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  26 loss :  tensor(0.1808, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  27 loss :  tensor(0.1696, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  28 loss :  tensor(0.1588, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  29 loss :  tensor(0.1588, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  30 loss :  tensor(0.1615, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  31 loss :  tensor(0.1458, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  32 loss :  tensor(0.1324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  33 loss :  tensor(0.1239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  34 loss :  tensor(0.1203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  35 loss :  tensor(0.1006, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  36 loss :  tensor(0.1145, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  37 loss :  tensor(0.0889, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  38 loss :  tensor(0.1037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  39 loss :  tensor(0.1097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  40 loss :  tensor(0.0953, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  41 loss :  tensor(0.0958, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  42 loss :  tensor(0.0882, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  43 loss :  tensor(0.0837, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  44 loss :  tensor(0.0735, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  45 loss :  tensor(0.0844, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  46 loss :  tensor(0.0651, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  47 loss :  tensor(0.0668, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  48 loss :  tensor(0.0651, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  49 loss :  tensor(0.0603, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "Epoch :  50 loss :  tensor(0.0560, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prediction**"
      ],
      "metadata": {
        "id": "09opIxXKQwxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(text):\n",
        "    #set the model to evaluation\n",
        "    model.eval()\n",
        "\n",
        "    #preprocess\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    #converting the text to word tokens\n",
        "    input_tokens = word_tokenize(text)\n",
        "\n",
        "    #converting the tokens to integer sequence\n",
        "    sequences = tokenizer.texts_to_sequences([input_tokens])\n",
        "\n",
        "    #converting to array\n",
        "    sequences=np.asarray(sequences)\n",
        "    with torch.no_grad():\n",
        "        #converting to tensor\n",
        "        sequences=torch.from_numpy(sequences)\n",
        "        #predicting the output\n",
        "        predict,(hidden,cell)=model(sequences)\n",
        "\n",
        "    #applying the softmax layer\n",
        "    softmax = torch.exp(predict)\n",
        "    prob = list(softmax.numpy())\n",
        "\n",
        "    #index of the predict word\n",
        "    predictions = np.argmax(prob)\n",
        "\n",
        "    #converting the sequence back to word\n",
        "    next_word=tokenizer.sequences_to_texts([[predictions]])\n",
        "    return next_word\n",
        ""
      ],
      "metadata": {
        "id": "9FPZN_X_Q0Vu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "F5wZtWWBRAQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we trained our model with sequence length of 2\n",
        "input_text=\"pale and\""
      ],
      "metadata": {
        "id": "j5lLqYq-Q5en"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Possible next word will be:\")\n",
        "predict_next_word(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcnHxvxJQ9i2",
        "outputId": "508aacbf-ef05-425d-a5a3-c2e8fb7928b5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible next word will be:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ales']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}